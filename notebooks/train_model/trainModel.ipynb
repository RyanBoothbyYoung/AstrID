{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f51c6a31-0548-4f9f-b1ca-2652f2caadde",
   "metadata": {},
   "source": [
    "## UnSu Unet Object Localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d8c9b41-da1a-431e-8133-f228618ec132",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 22:30:14.377458: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-17 22:30:14.407871: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-17 22:30:14.408279: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-17 22:30:15.039020: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.initializers import he_uniform\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b110441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom functions to extract our Image arrays and Pixel Mask arrays from our created fits files dataset\n",
    "from dataGathering import extractImageArray, extractPixelMaskArray, extract_star_catalog, getStarData\n",
    "\n",
    "# Import astropy to read fits files, and os to interact with the file system\n",
    "from astropy.io import fits\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b75e80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SkyView\n",
      "Vizier\n",
      "Save\n",
      "Save Catalog\n",
      "Number of stars in catalog query:  961\n",
      "Number of cataloged stars in image:  596\n",
      "Drawing\n",
      "SkyView\n",
      "Vizier\n",
      "Save\n",
      "Save Catalog\n",
      "Number of stars in catalog query:  844\n",
      "Number of cataloged stars in image:  552\n",
      "Drawing\n",
      "SkyView\n",
      "Vizier\n",
      "Save\n",
      "Save Catalog\n",
      "Number of stars in catalog query:  687\n",
      "Number of cataloged stars in image:  468\n",
      "Drawing\n",
      "SkyView\n",
      "Vizier\n",
      "Save\n",
      "Save Catalog\n",
      "Number of stars in catalog query:  1884\n",
      "Number of cataloged stars in image:  1273\n",
      "Drawing\n",
      "SkyView\n",
      "Vizier\n",
      "Save\n",
      "Save Catalog\n",
      "Number of stars in catalog query:  2207\n",
      "Number of cataloged stars in image:  1416\n",
      "Drawing\n",
      "SkyView\n",
      "Vizier\n",
      "Save\n",
      "Save Catalog\n",
      "Number of stars in catalog query:  735\n",
      "Number of cataloged stars in image:  474\n",
      "Drawing\n",
      "SkyView\n",
      "Vizier\n",
      "Save\n",
      "Save Catalog\n",
      "Number of stars in catalog query:  457\n",
      "Number of cataloged stars in image:  279\n",
      "Drawing\n",
      "SkyView\n",
      "Vizier\n",
      "Save\n",
      "Save Catalog\n",
      "Number of stars in catalog query:  1006\n",
      "Number of cataloged stars in image:  670\n",
      "Drawing\n",
      "SkyView\n",
      "Vizier\n",
      "Save\n",
      "Save Catalog\n",
      "Number of stars in catalog query:  1258\n",
      "Number of cataloged stars in image:  808\n",
      "Drawing\n",
      "SkyView\n",
      "Vizier\n",
      "Save\n",
      "Save Catalog\n",
      "Number of stars in catalog query:  2160\n",
      "Number of cataloged stars in image:  1396\n",
      "Drawing\n"
     ]
    }
   ],
   "source": [
    "getStarData('II/246', 10, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0ad1a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data7.fits added to dataset\n",
      "data4.fits added to dataset\n",
      "data5.fits added to dataset\n",
      "data8.fits added to dataset\n",
      "data1.fits added to dataset\n",
      "data9.fits added to dataset\n",
      "data6.fits added to dataset\n",
      "data3.fits added to dataset\n",
      "data0.fits added to dataset\n",
      "data2.fits added to dataset\n"
     ]
    }
   ],
   "source": [
    "# Create images and masks arrays lists\n",
    "images = []\n",
    "masks = []\n",
    "\n",
    "# Create df to store the star data inside each fits file\n",
    "star_data = []\n",
    "\n",
    "# Create a list of all the fits files in the dataset folder\n",
    "fits_files = os.listdir('data/')\n",
    "\n",
    "# For all the fits files in the dataset folder specified in file_path, extract the image and mask arrays to the respective lists\n",
    "file_path = 'data/'\n",
    "for file in os.listdir(file_path):\n",
    "    if file.endswith('.fits'):\n",
    "        images.append(extractImageArray(file_path + file))\n",
    "        masks.append(extractPixelMaskArray(file_path + file))\n",
    "        star_data.append(extract_star_catalog(file_path + file))\n",
    "\n",
    "        print(file + ' added to dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60dab5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c32043a-b07f-4121-abd4-45bbdd2db5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder path\n",
    "    #32 kernels\n",
    "    #3x3 kernel size\n",
    "    #padding = same considers edges in the input\n",
    "def unet_model(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "# Encoder path\n",
    "    c1 = layers.Conv2D(filters[0], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(inputs)\n",
    "    c1 = layers.Conv2D(filters[0], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c1)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer=he_uniform())(p1)\n",
    "    c2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer=he_uniform())(c2)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer=he_uniform())(p2)\n",
    "    c3 = layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer=he_uniform())(c3)\n",
    "    p3 = layers.MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    c4 = layers.Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer=he_uniform())(p3)\n",
    "    c4 = layers.Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer=he_uniform())(c4)\n",
    "    p4 = layers.MaxPooling2D((2, 2))(c4)\n",
    "\n",
    "    # Bottleneck\n",
    "    c5 = layers.Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer=he_uniform())(p4)\n",
    "    c5 = layers.Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer=he_uniform())(c5)\n",
    "\n",
    "    # Decoder path\n",
    "    u6 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u6 = layers.concatenate([u6, c4])\n",
    "    c6 = layers.Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer=he_uniform())(u6)\n",
    "    c6 = layers.Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer=he_uniform())(c6)\n",
    "\n",
    "    u7 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = layers.concatenate([u7, c3])\n",
    "    c7 = layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer=he_uniform())(u7)\n",
    "    c7 = layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer=he_uniform())(c7)\n",
    "\n",
    "    u8 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "    u8 = layers.concatenate([u8, c2])\n",
    "    c8 = layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer=he_uniform())(u8)\n",
    "    c8 = layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer=he_uniform())(c8)\n",
    "\n",
    "    u9 = layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "    u9 = layers.concatenate([u9, c1])\n",
    "    c9 = layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer=he_uniform())(u9)\n",
    "    c9 = layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer=he_uniform())(c9)\n",
    "\n",
    "    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(c9) # 512 x 512 x 1 filled with 1s or 0s\n",
    "\n",
    "    model = models.Model(inputs=[inputs], outputs=[outputs])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81b6d892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to numpy arrays\n",
    "# This ensures that the data is in the correct format for train_test_split and model.fit.\n",
    "train_images = np.array(images)\n",
    "train_masks = np.array(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "602598a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the input data has the correct shape\n",
    "if train_images.ndim == 3:  # If images are grayscale and have shape (num_samples, height, width)\n",
    "    train_images = np.expand_dims(train_images, axis=-1)  # Add channel dimension\n",
    "if train_masks.ndim == 3:  # If masks are grayscale and have shape (num_samples, height, width)\n",
    "    train_masks = np.expand_dims(train_masks, axis=-1)  # Add channel dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60f9aac3-773f-42a4-9ef7-445917129f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "train_images, val_images, train_masks, val_masks = train_test_split(train_images, train_masks, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ccedae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate channels for the grayscale images since the model expects 3 channels\n",
    "train_images = np.repeat(train_images, 3, axis=-1)\n",
    "val_images = np.repeat(val_images, 3, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "994391a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images:  (8, 512, 512, 3)\n",
      "Training masks:  (8, 512, 512, 1)\n",
      "Validation images:  (2, 512, 512, 3)\n",
      "Validation masks:  (2, 512, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "# Show distribution of training and validation sets\n",
    "print('Training images: ', train_images.shape)\n",
    "print('Training masks: ', train_masks.shape)\n",
    "print('Validation images: ', val_images.shape)\n",
    "print('Validation masks: ', val_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db7ad3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 22:31:24.555374: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-17 22:31:24.555805: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# Create and compile the model\n",
    "input_shape = (512, 512, 3)\n",
    "model = unet_model(input_shape)\n",
    "model.compile(\n",
    "optimizer='adam', \n",
    "    loss='binary_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c847e17-f0ad-4354-9dff-62a7cd6bcda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 22:31:37.362875: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 536870912 exceeds 10% of free system memory.\n",
      "2024-10-17 22:31:39.860289: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 603979776 exceeds 10% of free system memory.\n",
      "2024-10-17 22:31:39.864850: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 536870912 exceeds 10% of free system memory.\n",
      "2024-10-17 22:31:39.864922: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 603979776 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 21s 21s/step - loss: 1652.7817 - accuracy: 0.2425 - val_loss: 31.7011 - val_accuracy: 0.9674\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 22:31:55.926010: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 536870912 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 17s 17s/step - loss: 58.9418 - accuracy: 0.9642 - val_loss: 15.4374 - val_accuracy: 0.9978\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 16s 16s/step - loss: 37.2845 - accuracy: 0.9966 - val_loss: 18.3671 - val_accuracy: 0.9980\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 14s 14s/step - loss: 43.9654 - accuracy: 0.9967 - val_loss: 19.7028 - val_accuracy: 0.9980\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 14s 14s/step - loss: 46.9025 - accuracy: 0.9967 - val_loss: 19.7645 - val_accuracy: 0.9980\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 14s 14s/step - loss: 46.9793 - accuracy: 0.9967 - val_loss: 18.5095 - val_accuracy: 0.9980\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 14s 14s/step - loss: 43.9946 - accuracy: 0.9967 - val_loss: 16.9386 - val_accuracy: 0.9980\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 14s 14s/step - loss: 40.2044 - accuracy: 0.9967 - val_loss: 15.5189 - val_accuracy: 0.9980\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 14s 14s/step - loss: 36.7557 - accuracy: 0.9967 - val_loss: 14.1117 - val_accuracy: 0.9980\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 14s 14s/step - loss: 33.3344 - accuracy: 0.9967 - val_loss: 12.5032 - val_accuracy: 0.9980\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 13s 13s/step - loss: 29.4388 - accuracy: 0.9967 - val_loss: 10.6708 - val_accuracy: 0.9980\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 14s 14s/step - loss: 24.9835 - accuracy: 0.9967 - val_loss: 8.7073 - val_accuracy: 0.9979\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 14s 14s/step - loss: 20.2149 - accuracy: 0.9966 - val_loss: 7.3767 - val_accuracy: 0.9961\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 14s 14s/step - loss: 16.6122 - accuracy: 0.9940 - val_loss: 6.0665 - val_accuracy: 0.9958\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 14s 14s/step - loss: 13.5312 - accuracy: 0.9935 - val_loss: 5.0772 - val_accuracy: 0.9970\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 14s 14s/step - loss: 11.4625 - accuracy: 0.9954 - val_loss: 4.2396 - val_accuracy: 0.9971\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 14s 14s/step - loss: 9.7205 - accuracy: 0.9952 - val_loss: 3.7681 - val_accuracy: 0.9926\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 14s 14s/step - loss: 8.4136 - accuracy: 0.9895 - val_loss: 3.4933 - val_accuracy: 0.9928\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 15s 15s/step - loss: 7.6719 - accuracy: 0.9903 - val_loss: 3.0642 - val_accuracy: 0.9976\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 14s 14s/step - loss: 7.1576 - accuracy: 0.9963 - val_loss: 3.1903 - val_accuracy: 0.9973\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 14s 14s/step - loss: 7.4664 - accuracy: 0.9959 - val_loss: 3.1355 - val_accuracy: 0.9963\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 13s 13s/step - loss: 7.2722 - accuracy: 0.9946 - val_loss: 2.7683 - val_accuracy: 0.9968\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 14s 14s/step - loss: 6.5134 - accuracy: 0.9952 - val_loss: 2.3406 - val_accuracy: 0.9971\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 14s 14s/step - loss: 5.5827 - accuracy: 0.9957 - val_loss: 2.0575 - val_accuracy: 0.9955\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 14s 14s/step - loss: 4.8084 - accuracy: 0.9935 - val_loss: 2.2218 - val_accuracy: 0.9891\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 16s 16s/step - loss: 4.8838 - accuracy: 0.9855 - val_loss: 1.6481 - val_accuracy: 0.9953\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 17s 17s/step - loss: 3.8907 - accuracy: 0.9926 - val_loss: 1.5202 - val_accuracy: 0.9968\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 15s 15s/step - loss: 3.6381 - accuracy: 0.9950 - val_loss: 1.6535 - val_accuracy: 0.9956\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 14s 14s/step - loss: 3.7729 - accuracy: 0.9935 - val_loss: 1.9632 - val_accuracy: 0.9939\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 15s 15s/step - loss: 4.2898 - accuracy: 0.9909 - val_loss: 1.6172 - val_accuracy: 0.9974\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 14s 14s/step - loss: 3.8664 - accuracy: 0.9959 - val_loss: 1.6139 - val_accuracy: 0.9978\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 15s 15s/step - loss: 3.9010 - accuracy: 0.9964 - val_loss: 1.4940 - val_accuracy: 0.9976\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 14s 14s/step - loss: 3.6062 - accuracy: 0.9962 - val_loss: 1.3856 - val_accuracy: 0.9953\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 15s 15s/step - loss: 3.2557 - accuracy: 0.9929 - val_loss: 1.4549 - val_accuracy: 0.9914\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 15s 15s/step - loss: 3.2590 - accuracy: 0.9875 - val_loss: 1.2034 - val_accuracy: 0.9960\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 16s 16s/step - loss: 2.8692 - accuracy: 0.9938 - val_loss: 1.1878 - val_accuracy: 0.9965\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 15s 15s/step - loss: 2.8399 - accuracy: 0.9947 - val_loss: 1.2155 - val_accuracy: 0.9950\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 15s 15s/step - loss: 2.8347 - accuracy: 0.9927 - val_loss: 1.2746 - val_accuracy: 0.9930\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 15s 15s/step - loss: 2.8466 - accuracy: 0.9901 - val_loss: 1.1534 - val_accuracy: 0.9954\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 15s 15s/step - loss: 2.6639 - accuracy: 0.9935 - val_loss: 1.0901 - val_accuracy: 0.9969\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 15s 15s/step - loss: 2.6190 - accuracy: 0.9953 - val_loss: 1.0341 - val_accuracy: 0.9958\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 15s 15s/step - loss: 2.4171 - accuracy: 0.9940 - val_loss: 0.9790 - val_accuracy: 0.9944\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 15s 15s/step - loss: 2.2379 - accuracy: 0.9922 - val_loss: 0.9359 - val_accuracy: 0.9942\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 15s 15s/step - loss: 2.1371 - accuracy: 0.9926 - val_loss: 1.0513 - val_accuracy: 0.9915\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 15s 15s/step - loss: 2.2632 - accuracy: 0.9907 - val_loss: 0.9170 - val_accuracy: 0.9974\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 15s 15s/step - loss: 2.2606 - accuracy: 0.9960 - val_loss: 0.8940 - val_accuracy: 0.9966\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 16s 16s/step - loss: 2.2048 - accuracy: 0.9952 - val_loss: 0.8511 - val_accuracy: 0.9932\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 15s 15s/step - loss: 2.0122 - accuracy: 0.9915 - val_loss: 1.7361 - val_accuracy: 0.9714\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 16s 16s/step - loss: 2.9420 - accuracy: 0.9735 - val_loss: 1.2344 - val_accuracy: 0.9978\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 20s 20s/step - loss: 3.0923 - accuracy: 0.9966 - val_loss: 1.7269 - val_accuracy: 0.9980\n",
      "Epoch 51/100\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images, train_masks, \n",
    "                    validation_data=(val_images, val_masks), \n",
    "                    epochs=100, \n",
    "                    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b431a9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322ff4ef-a8b3-4b9f-af7b-cdf0c5437e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Set a larger figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.plot(history.history['loss'], color='blue', linestyle='-', linewidth=2, label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], color='orange', linestyle='--', linewidth=2, label='Validation Loss')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Training and Validation Loss Over Epochs', fontsize=16)\n",
    "plt.xlabel('Epochs', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "\n",
    "# Add a grid for better readability\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add legend to differentiate between training and validation loss\n",
    "plt.legend(loc='best', fontsize=12)\n",
    "\n",
    "# Set limits for better visualization\n",
    "plt.xlim(0, len(history.history['loss']) - 1)  # From epoch 0 to the last epoch\n",
    "plt.ylim(min(history.history['loss']) * 0.95, max(history.history['val_loss']) * 1.05)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
