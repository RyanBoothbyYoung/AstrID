{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f51c6a31-0548-4f9f-b1ca-2652f2caadde",
   "metadata": {},
   "source": [
    "## UnSu Unet Object Localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d8c9b41-da1a-431e-8133-f228618ec132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.initializers import he_uniform\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d260a505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b110441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom functions to extract our Image arrays and Pixel Mask arrays from our created fits files dataset\n",
    "from dataGathering import extractImageArray, extractPixelMaskArray, extract_star_catalog, getStarData, getImagePlot, getPixelMaskPlot, displayImagePlot, displayPixelMaskPlot\n",
    "\n",
    "# Import astropy to read fits files, and os to interact with the file system\n",
    "from astropy.io import fits\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b75e80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getStarData('II/246', 150, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0ad1a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data0.fits added to dataset\n",
      "data1.fits added to dataset\n",
      "data10.fits added to dataset\n",
      "data100.fits added to dataset\n",
      "data101.fits added to dataset\n",
      "data102.fits added to dataset\n",
      "data103.fits added to dataset\n",
      "data104.fits added to dataset\n",
      "data105.fits added to dataset\n",
      "data106.fits added to dataset\n",
      "data107.fits added to dataset\n",
      "data108.fits added to dataset\n",
      "data109.fits added to dataset\n",
      "data11.fits added to dataset\n",
      "data110.fits added to dataset\n",
      "data111.fits added to dataset\n",
      "data112.fits added to dataset\n",
      "data113.fits added to dataset\n",
      "data114.fits added to dataset\n",
      "data115.fits added to dataset\n",
      "data116.fits added to dataset\n",
      "data117.fits added to dataset\n",
      "data118.fits added to dataset\n",
      "data119.fits added to dataset\n",
      "data12.fits added to dataset\n",
      "data120.fits added to dataset\n",
      "data121.fits added to dataset\n",
      "data122.fits added to dataset\n",
      "data123.fits added to dataset\n",
      "data124.fits added to dataset\n",
      "data125.fits added to dataset\n",
      "data126.fits added to dataset\n",
      "data127.fits added to dataset\n",
      "data128.fits added to dataset\n",
      "data129.fits added to dataset\n",
      "data13.fits added to dataset\n",
      "data130.fits added to dataset\n",
      "data131.fits added to dataset\n",
      "data132.fits added to dataset\n",
      "data133.fits added to dataset\n",
      "data134.fits added to dataset\n",
      "data135.fits added to dataset\n",
      "data136.fits added to dataset\n",
      "data137.fits added to dataset\n",
      "data138.fits added to dataset\n",
      "data139.fits added to dataset\n",
      "data14.fits added to dataset\n",
      "data140.fits added to dataset\n",
      "data141.fits added to dataset\n",
      "data142.fits added to dataset\n",
      "data143.fits added to dataset\n",
      "data144.fits added to dataset\n",
      "data145.fits added to dataset\n",
      "data146.fits added to dataset\n",
      "data147.fits added to dataset\n",
      "data148.fits added to dataset\n",
      "data149.fits added to dataset\n",
      "data15.fits added to dataset\n",
      "data16.fits added to dataset\n",
      "data17.fits added to dataset\n",
      "data18.fits added to dataset\n",
      "data19.fits added to dataset\n",
      "data2.fits added to dataset\n",
      "data20.fits added to dataset\n",
      "data21.fits added to dataset\n",
      "data22.fits added to dataset\n",
      "data23.fits added to dataset\n",
      "data24.fits added to dataset\n",
      "data25.fits added to dataset\n",
      "data26.fits added to dataset\n",
      "data27.fits added to dataset\n",
      "data28.fits added to dataset\n",
      "data29.fits added to dataset\n",
      "data3.fits added to dataset\n",
      "data30.fits added to dataset\n",
      "data31.fits added to dataset\n",
      "data32.fits added to dataset\n",
      "data33.fits added to dataset\n",
      "data34.fits added to dataset\n",
      "data35.fits added to dataset\n",
      "data36.fits added to dataset\n",
      "data37.fits added to dataset\n",
      "data38.fits added to dataset\n",
      "data39.fits added to dataset\n",
      "data4.fits added to dataset\n",
      "data40.fits added to dataset\n",
      "data41.fits added to dataset\n",
      "data42.fits added to dataset\n",
      "data43.fits added to dataset\n",
      "data44.fits added to dataset\n",
      "data45.fits added to dataset\n",
      "data46.fits added to dataset\n",
      "data47.fits added to dataset\n",
      "data48.fits added to dataset\n",
      "data49.fits added to dataset\n",
      "data5.fits added to dataset\n",
      "data50.fits added to dataset\n",
      "data51.fits added to dataset\n",
      "data52.fits added to dataset\n",
      "data53.fits added to dataset\n",
      "data54.fits added to dataset\n",
      "data55.fits added to dataset\n",
      "data56.fits added to dataset\n",
      "data57.fits added to dataset\n",
      "data58.fits added to dataset\n",
      "data59.fits added to dataset\n",
      "data6.fits added to dataset\n",
      "data60.fits added to dataset\n",
      "data61.fits added to dataset\n",
      "data62.fits added to dataset\n",
      "data63.fits added to dataset\n",
      "data64.fits added to dataset\n",
      "data65.fits added to dataset\n",
      "data66.fits added to dataset\n",
      "data67.fits added to dataset\n",
      "data68.fits added to dataset\n",
      "data69.fits added to dataset\n",
      "data7.fits added to dataset\n",
      "data70.fits added to dataset\n",
      "data71.fits added to dataset\n",
      "data72.fits added to dataset\n",
      "data73.fits added to dataset\n",
      "data74.fits added to dataset\n",
      "data75.fits added to dataset\n",
      "data76.fits added to dataset\n",
      "data77.fits added to dataset\n",
      "data78.fits added to dataset\n",
      "data79.fits added to dataset\n",
      "data8.fits added to dataset\n",
      "data80.fits added to dataset\n",
      "data81.fits added to dataset\n",
      "data82.fits added to dataset\n",
      "data83.fits added to dataset\n",
      "data84.fits added to dataset\n",
      "data85.fits added to dataset\n",
      "data86.fits added to dataset\n",
      "data87.fits added to dataset\n",
      "data88.fits added to dataset\n",
      "data89.fits added to dataset\n",
      "data9.fits added to dataset\n",
      "data90.fits added to dataset\n",
      "data91.fits added to dataset\n",
      "data92.fits added to dataset\n",
      "data93.fits added to dataset\n",
      "data94.fits added to dataset\n",
      "data95.fits added to dataset\n",
      "data96.fits added to dataset\n",
      "data97.fits added to dataset\n",
      "data98.fits added to dataset\n",
      "data99.fits added to dataset\n"
     ]
    }
   ],
   "source": [
    "# Create images and masks arrays lists\n",
    "images = []\n",
    "masks = []\n",
    "\n",
    "# Create df to store the star data inside each fits file\n",
    "star_data = []\n",
    "\n",
    "# Create a list of all the fits files in the dataset folder\n",
    "fits_files = os.listdir('data/')\n",
    "\n",
    "# For all the fits files in the dataset folder specified in file_path, extract the image and mask arrays to the respective lists\n",
    "file_path = 'data/'\n",
    "for file in os.listdir(file_path):\n",
    "    if file.endswith('.fits'):\n",
    "        images.append(extractImageArray(file_path + file))\n",
    "        masks.append(extractPixelMaskArray(file_path + file))\n",
    "        star_data.append(extract_star_catalog(file_path + file))\n",
    "\n",
    "        print(file + ' added to dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60dab5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c32043a-b07f-4121-abd4-45bbdd2db5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder path\n",
    "    #32 kernels\n",
    "    #3x3 kernel size\n",
    "    #padding = same considers edges in the input\n",
    "# Example function to create a U-Net model\n",
    "def unet_model(input_shape, filters, kernel_size, activation, padding, initializer):\n",
    "    inputs = layers.Input(input_shape)\n",
    "\n",
    "    # Encoder path\n",
    "    c1 = layers.Conv2D(filters[0], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(inputs)\n",
    "    c1 = layers.Conv2D(filters[0], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c1)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = layers.Conv2D(filters[1], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(p1)\n",
    "    c2 = layers.Conv2D(filters[1], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c2)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = layers.Conv2D(filters[2], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(p2)\n",
    "    c3 = layers.Conv2D(filters[2], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c3)\n",
    "    p3 = layers.MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    c4 = layers.Conv2D(filters[3], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(p3)\n",
    "    c4 = layers.Conv2D(filters[3], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c4)\n",
    "    p4 = layers.MaxPooling2D((2, 2))(c4)\n",
    "\n",
    "    c5 = layers.Conv2D(filters[4], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(p4)\n",
    "    c5 = layers.Conv2D(filters[4], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c5)\n",
    "\n",
    "    # Decoder path\n",
    "    u6 = layers.Conv2DTranspose(filters[3], (2, 2), strides=(2, 2), padding=padding)(c5)\n",
    "    u6 = layers.concatenate([u6, c4])\n",
    "    c6 = layers.Conv2D(filters[3], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(u6)\n",
    "    c6 = layers.Conv2D(filters[3], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c6)\n",
    "\n",
    "    u7 = layers.Conv2DTranspose(filters[2], (2, 2), strides=(2, 2), padding=padding)(c6)\n",
    "    u7 = layers.concatenate([u7, c3])\n",
    "    c7 = layers.Conv2D(filters[2], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(u7)\n",
    "    c7 = layers.Conv2D(filters[2], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c7)\n",
    "\n",
    "    u8 = layers.Conv2DTranspose(filters[1], (2, 2), strides=(2, 2), padding=padding)(c7)\n",
    "    u8 = layers.concatenate([u8, c2])\n",
    "    c8 = layers.Conv2D(filters[1], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(u8)\n",
    "    c8 = layers.Conv2D(filters[1], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c8)\n",
    "\n",
    "    u9 = layers.Conv2DTranspose(filters[0], (2, 2), strides=(2, 2), padding=padding)(c8)\n",
    "    u9 = layers.concatenate([u9, c1])\n",
    "    c9 = layers.Conv2D(filters[0], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(u9)\n",
    "    c9 = layers.Conv2D(filters[0], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c9)\n",
    "\n",
    "    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "\n",
    "    model = models.Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f66df499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def weightedBinaryCrossEntropy(y_test, y_pred, weights=[2.0, 1.0]):\n",
    "\n",
    "    # Debugging\n",
    "    # print('**** Calculating Weighted Binary Cross Entroy ****')\n",
    "    # print('* ytest:', type(y_test), y_test)\n",
    "    # print('* ypred:', type(y_pred), y_pred)\n",
    "\n",
    "    bce = weights * y_test  * tf.math.log(y_pred * K.epsilon)\n",
    "    bce += weights[0] * (1 -y_test) * tf.math.log(1 - y_pred * K.epsilon)\n",
    "\n",
    "    # print('* calcuated WBCE:', bce)\n",
    "    return bce\n",
    "\n",
    "    # weights = (y_test * 59.) + 1.\n",
    "    # bce = K.binary_crossentropy(y_test, y_pred)\n",
    "    # weighted_bce = K.mean(bce * weights)\n",
    "    # return weighted_bce\n",
    "\n",
    "\n",
    "# **** Calculating Weighted Binary Cross Entroy ****\n",
    "# * ytest: <class 'tensorflow.python.framework.ops.SymbolicTensor'> Tensor(\"data_1:0\", shape=(4, 512, 512, 1), dtype=float32)\n",
    "# * ypred: <class 'tensorflow.python.framework.ops.SymbolicTensor'> Tensor(\"functional_3_1/conv2d_75_1/Sigmoid:0\", shape=(4, 512, 512, 1), dtype=float32)\n",
    "# * calcuated WBCE: Tensor(\"compile_loss/weighted_binary_cross_entropy/mul:0\", shape=(4, 512, 512, 2), dtype=float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8beae7fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "weightedBinaryCrossEntropy() missing 2 required positional arguments: 'y_test' and 'y_pred'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 13\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Prepare the model for training\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#############################################\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Define hyperparameters\u001b[39;00m\n\u001b[0;32m      5\u001b[0m hyperparameters \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_shape\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m3\u001b[39m),\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilters\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m512\u001b[39m],\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel_size\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m),\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitializer\u001b[39m\u001b[38;5;124m'\u001b[39m: he_uniform(),\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mweightedBinaryCrossEntropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# 'loss': 'binary_crossentropy',\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation_split\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mearly_stopping_patience\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m     20\u001b[0m }\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Create and compile the model using hyperparameters\u001b[39;00m\n\u001b[0;32m     23\u001b[0m model \u001b[38;5;241m=\u001b[39m unet_model(\n\u001b[0;32m     24\u001b[0m     input_shape\u001b[38;5;241m=\u001b[39mhyperparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_shape\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     25\u001b[0m     filters\u001b[38;5;241m=\u001b[39mhyperparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilters\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     initializer\u001b[38;5;241m=\u001b[39mhyperparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitializer\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     30\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: weightedBinaryCrossEntropy() missing 2 required positional arguments: 'y_test' and 'y_pred'"
     ]
    }
   ],
   "source": [
    "# Prepare the model for training\n",
    "#############################################\n",
    "\n",
    "# Define hyperparameters\n",
    "hyperparameters = {\n",
    "    'input_shape': (512, 512, 3),\n",
    "    'filters': [32, 64, 128, 256, 512],\n",
    "    'kernel_size': (3, 3),\n",
    "    'activation': 'relu',\n",
    "    'padding': 'same',\n",
    "    'initializer': he_uniform(),\n",
    "    'optimizer': 'adam',\n",
    "    'loss': weightedBinaryCrossEntropy(),\n",
    "    # 'loss': 'binary_crossentropy',\n",
    "    'metrics': ['accuracy'],\n",
    "    'epochs': 1,\n",
    "    'batch_size': 4,\n",
    "    'validation_split': 0.2,\n",
    "    'early_stopping_patience': 10\n",
    "}\n",
    "\n",
    "# Create and compile the model using hyperparameters\n",
    "model = unet_model(\n",
    "    input_shape=hyperparameters['input_shape'],\n",
    "    filters=hyperparameters['filters'],\n",
    "    kernel_size=hyperparameters['kernel_size'],\n",
    "    activation=hyperparameters['activation'],\n",
    "    padding=hyperparameters['padding'],\n",
    "    initializer=hyperparameters['initializer']\n",
    ")\n",
    "model.compile(\n",
    "    optimizer=hyperparameters['optimizer'],\n",
    "    loss=hyperparameters['loss'],\n",
    "    metrics=hyperparameters['metrics']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c1e6c296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images:  (120, 512, 512, 3)\n",
      "Training masks:  (120, 512, 512, 1)\n",
      "Validation images:  (30, 512, 512, 3)\n",
      "Validation masks:  (30, 512, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data for training\n",
    "#############################################\n",
    "\n",
    "# Define the test size\n",
    "image_parameters = {\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 0\n",
    "}\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "# This ensures that the data is in the correct format for train_test_split and model.fit.\n",
    "train_images = np.array(images)\n",
    "train_masks = np.array(masks)\n",
    "\n",
    "\n",
    "\n",
    "# Ensure the input data has the correct shape\n",
    "if train_images.ndim == 3: # If images are grayscale and have shape (num_samples, height, width)\n",
    "    train_images = np.expand_dims(train_images, axis=-1) # Add channel dimension\n",
    "if train_masks.ndim == 3: # If masks are grayscale and have shape (num_samples, height, width)\n",
    "    train_masks = np.expand_dims(train_masks, axis=-1) # Add channel dimension\n",
    "\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_images, val_images, train_masks, val_masks = train_test_split(train_images, train_masks, test_size=image_parameters['test_size'], random_state=image_parameters['random_state'])\n",
    "\n",
    "# Duplicate channels for the grayscale images since the model expects 3 channels\n",
    "train_images = np.repeat(train_images, 3, axis=-1)\n",
    "val_images = np.repeat(val_images, 3, axis=-1)\n",
    "\n",
    "# Use ImageDataGenerator to load data in batches\n",
    "train_datagen = ImageDataGenerator()\n",
    "val_datagen = ImageDataGenerator()\n",
    "\n",
    "train_generator = train_datagen.flow(train_images, train_masks, batch_size=hyperparameters['batch_size'])\n",
    "val_generator = val_datagen.flow(val_images, val_masks, batch_size=hyperparameters['batch_size'])\n",
    "\n",
    "# Show distribution of training and validation sets\n",
    "print('Training images: ', train_images.shape)\n",
    "print('Training masks: ', train_masks.shape)\n",
    "print('Validation images: ', val_images.shape)\n",
    "print('Validation masks: ', val_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851767c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 7/30\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:17\u001b[0m 9s/step - accuracy: 0.9662 - loss: 32.4392"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "#############################################\n",
    "\n",
    "# Implement Early stopping to cut useless epochs\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # or another metric like 'val_accuracy'\n",
    "    patience=hyperparameters['early_stopping_patience'],         # Number of epochs with no improvement before stopping\n",
    "    restore_best_weights=True  # Restores the model to the best state after stopping\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_images, train_masks, \n",
    "                    validation_data=(val_images, val_masks), \n",
    "                    epochs=hyperparameters['epochs'], \n",
    "                    batch_size=hyperparameters['batch_size'],\n",
    "                    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9056adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show training loss and validation loss\n",
    "print('Training loss: ', history.history['loss'][-1])\n",
    "print('Validation loss: ', history.history['val_loss'][-1])\n",
    "\n",
    "# Show training accuracy and validation accuracy\n",
    "print('Training accuracy: ', history.history['accuracy'][-1])\n",
    "print('Validation accuracy: ', history.history['val_accuracy'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b431a9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(24, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1af23663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('150unet_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322ff4ef-a8b3-4b9f-af7b-cdf0c5437e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Set a larger figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.plot(history.history['loss'], color='blue', linestyle='-', linewidth=2, label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], color='orange', linestyle='--', linewidth=2, label='Validation Loss')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Training and Validation Loss Over Epochs', fontsize=16)\n",
    "plt.xlabel('Epochs', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "\n",
    "# Add a grid for better readability\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add legend to differentiate between training and validation loss\n",
    "plt.legend(loc='best', fontsize=12)\n",
    "\n",
    "# Set limits for better visualization\n",
    "plt.xlim(0, len(history.history['loss']) - 1)  # From epoch 0 to the last epoch\n",
    "plt.ylim(min(history.history['loss']) * 0.95, max(history.history['val_loss']) * 1.05)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae06b48f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639116f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
