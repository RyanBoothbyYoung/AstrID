{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f51c6a31-0548-4f9f-b1ca-2652f2caadde",
   "metadata": {},
   "source": [
    "## UnSu Unet Object Localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d8c9b41-da1a-431e-8133-f228618ec132",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 23:15:53.421666: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-17 23:15:53.611107: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-17 23:15:53.612431: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-17 23:15:54.709880: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.initializers import he_uniform\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b110441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom functions to extract our Image arrays and Pixel Mask arrays from our created fits files dataset\n",
    "from dataGathering import extractImageArray, extractPixelMaskArray, extract_star_catalog, getStarData\n",
    "\n",
    "# Import astropy to read fits files, and os to interact with the file system\n",
    "from astropy.io import fits\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b75e80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SkyView\n",
      "Vizier\n",
      "Save\n",
      "Save Catalog\n",
      "Number of stars in catalog query:  593\n",
      "Number of cataloged stars in image:  392\n",
      "Drawing\n",
      "SkyView\n",
      "Vizier\n",
      "Save\n",
      "Save Catalog\n",
      "Number of stars in catalog query:  1424\n",
      "Number of cataloged stars in image:  952\n",
      "Drawing\n",
      "SkyView\n",
      "Vizier\n",
      "Save\n",
      "Save Catalog\n",
      "Number of stars in catalog query:  701\n",
      "Number of cataloged stars in image:  434\n",
      "Drawing\n",
      "SkyView\n",
      "Vizier\n",
      "Save\n",
      "Save Catalog\n",
      "Number of stars in catalog query:  3554\n",
      "Number of cataloged stars in image:  2306\n",
      "Drawing\n",
      "SkyView\n",
      "Vizier\n",
      "Save\n",
      "Save Catalog\n",
      "Number of stars in catalog query:  2454\n",
      "Number of cataloged stars in image:  1571\n",
      "Drawing\n",
      "SkyView\n",
      "Vizier\n",
      "Save\n",
      "Save Catalog\n",
      "Number of stars in catalog query:  600\n",
      "Number of cataloged stars in image:  374\n",
      "Drawing\n",
      "SkyView\n",
      "Vizier\n",
      "Save\n",
      "Save Catalog\n",
      "Number of stars in catalog query:  2292\n",
      "Number of cataloged stars in image:  1507\n",
      "Drawing\n",
      "SkyView\n",
      "Vizier\n",
      "Save\n",
      "Save Catalog\n",
      "Number of stars in catalog query:  5970\n",
      "Number of cataloged stars in image:  3904\n",
      "Drawing\n",
      "SkyView\n",
      "Vizier\n",
      "Save\n",
      "Save Catalog\n",
      "Number of stars in catalog query:  1360\n",
      "Number of cataloged stars in image:  901\n",
      "Drawing\n",
      "SkyView\n",
      "Vizier\n",
      "Save\n",
      "Save Catalog\n",
      "Number of stars in catalog query:  716\n",
      "Number of cataloged stars in image:  462\n",
      "Drawing\n"
     ]
    }
   ],
   "source": [
    "# getStarData('II/246', 10, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0ad1a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data7.fits added to dataset\n",
      "data4.fits added to dataset\n",
      "data5.fits added to dataset\n",
      "data8.fits added to dataset\n",
      "data1.fits added to dataset\n",
      "data9.fits added to dataset\n",
      "data6.fits added to dataset\n",
      "data3.fits added to dataset\n",
      "data0.fits added to dataset\n",
      "data2.fits added to dataset\n"
     ]
    }
   ],
   "source": [
    "# Create images and masks arrays lists\n",
    "images = []\n",
    "masks = []\n",
    "\n",
    "# Create df to store the star data inside each fits file\n",
    "star_data = []\n",
    "\n",
    "# Create a list of all the fits files in the dataset folder\n",
    "fits_files = os.listdir('data/')\n",
    "\n",
    "# For all the fits files in the dataset folder specified in file_path, extract the image and mask arrays to the respective lists\n",
    "file_path = 'data/'\n",
    "for file in os.listdir(file_path):\n",
    "    if file.endswith('.fits'):\n",
    "        images.append(extractImageArray(file_path + file))\n",
    "        masks.append(extractPixelMaskArray(file_path + file))\n",
    "        star_data.append(extract_star_catalog(file_path + file))\n",
    "\n",
    "        print(file + ' added to dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60dab5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c32043a-b07f-4121-abd4-45bbdd2db5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder path\n",
    "    #32 kernels\n",
    "    #3x3 kernel size\n",
    "    #padding = same considers edges in the input\n",
    "# Example function to create a U-Net model\n",
    "def unet_model(input_shape, filters, kernel_size, activation, padding, initializer):\n",
    "    inputs = layers.Input(input_shape)\n",
    "\n",
    "    # Encoder path\n",
    "    c1 = layers.Conv2D(filters[0], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(inputs)\n",
    "    c1 = layers.Conv2D(filters[0], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c1)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = layers.Conv2D(filters[1], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(p1)\n",
    "    c2 = layers.Conv2D(filters[1], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c2)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = layers.Conv2D(filters[2], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(p2)\n",
    "    c3 = layers.Conv2D(filters[2], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c3)\n",
    "    p3 = layers.MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    c4 = layers.Conv2D(filters[3], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(p3)\n",
    "    c4 = layers.Conv2D(filters[3], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c4)\n",
    "    p4 = layers.MaxPooling2D((2, 2))(c4)\n",
    "\n",
    "    c5 = layers.Conv2D(filters[4], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(p4)\n",
    "    c5 = layers.Conv2D(filters[4], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c5)\n",
    "\n",
    "    # Decoder path\n",
    "    u6 = layers.Conv2DTranspose(filters[3], (2, 2), strides=(2, 2), padding=padding)(c5)\n",
    "    u6 = layers.concatenate([u6, c4])\n",
    "    c6 = layers.Conv2D(filters[3], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(u6)\n",
    "    c6 = layers.Conv2D(filters[3], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c6)\n",
    "\n",
    "    u7 = layers.Conv2DTranspose(filters[2], (2, 2), strides=(2, 2), padding=padding)(c6)\n",
    "    u7 = layers.concatenate([u7, c3])\n",
    "    c7 = layers.Conv2D(filters[2], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(u7)\n",
    "    c7 = layers.Conv2D(filters[2], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c7)\n",
    "\n",
    "    u8 = layers.Conv2DTranspose(filters[1], (2, 2), strides=(2, 2), padding=padding)(c7)\n",
    "    u8 = layers.concatenate([u8, c2])\n",
    "    c8 = layers.Conv2D(filters[1], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(u8)\n",
    "    c8 = layers.Conv2D(filters[1], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c8)\n",
    "\n",
    "    u9 = layers.Conv2DTranspose(filters[0], (2, 2), strides=(2, 2), padding=padding)(c8)\n",
    "    u9 = layers.concatenate([u9, c1])\n",
    "    c9 = layers.Conv2D(filters[0], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(u9)\n",
    "    c9 = layers.Conv2D(filters[0], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c9)\n",
    "\n",
    "    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "\n",
    "    model = models.Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8beae7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 23:17:01.853141: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-17 23:17:01.853599: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "/home/chris/GitHub/AstrID/.venv/lib/python3.10/site-packages/keras/initializers/initializers.py:120: UserWarning: The initializer HeUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Prepare the model for training\n",
    "#############################################\n",
    "\n",
    "# Define hyperparameters\n",
    "hyperparameters = {\n",
    "    'input_shape': (512, 512, 3),\n",
    "    'filters': [32, 64, 128, 256, 512],\n",
    "    'kernel_size': (3, 3),\n",
    "    'activation': 'relu',\n",
    "    'padding': 'same',\n",
    "    'initializer': he_uniform(),\n",
    "    'optimizer': 'adam',\n",
    "    'loss': 'binary_crossentropy',\n",
    "    'metrics': ['accuracy'],\n",
    "    'epochs': 5,\n",
    "    'batch_size': 32,\n",
    "    'validation_split': 0.2,\n",
    "    'early_stopping_patience': 10\n",
    "}\n",
    "\n",
    "# Create and compile the model using hyperparameters\n",
    "model = unet_model(\n",
    "    input_shape=hyperparameters['input_shape'],\n",
    "    filters=hyperparameters['filters'],\n",
    "    kernel_size=hyperparameters['kernel_size'],\n",
    "    activation=hyperparameters['activation'],\n",
    "    padding=hyperparameters['padding'],\n",
    "    initializer=hyperparameters['initializer']\n",
    ")\n",
    "model.compile(\n",
    "    optimizer=hyperparameters['optimizer'],\n",
    "    loss=hyperparameters['loss'],\n",
    "    metrics=hyperparameters['metrics']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1e6c296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images:  (8, 512, 512, 3)\n",
      "Training masks:  (8, 512, 512, 1)\n",
      "Validation images:  (2, 512, 512, 3)\n",
      "Validation masks:  (2, 512, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data for training\n",
    "#############################################\n",
    "\n",
    "# Define the test size\n",
    "image_parameters = {\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 0\n",
    "}\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "# This ensures that the data is in the correct format for train_test_split and model.fit.\n",
    "train_images = np.array(images)\n",
    "train_masks = np.array(masks)\n",
    "\n",
    "# Duplicate channels for the grayscale images since the model expects 3 channels\n",
    "train_images = np.expand_dims(train_images, axis=3)\n",
    "train_masks = np.expand_dims(train_masks, axis=3)\n",
    "\n",
    "# Ensure the input data has the correct shape\n",
    "if train_images.ndim == 3: # If images are grayscale and have shape (num_samples, height, width)\n",
    "    train_images = np.expand_dims(train_images, axis=-1) # Add channel dimension\n",
    "if train_masks.ndim == 3: # If masks are grayscale and have shape (num_samples, height, width)\n",
    "    train_masks = np.expand_dims(train_masks, axis=-1) # Add channel dimension\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_images, val_images, train_masks, val_masks = train_test_split(train_images, train_masks, test_size=image_parameters['test_size'], random_state=image_parameters['random_state'])\n",
    "\n",
    "# Duplicate channels for the grayscale images since the model expects 3 channels\n",
    "train_images = np.repeat(train_images, 3, axis=-1)\n",
    "val_images = np.repeat(val_images, 3, axis=-1)\n",
    "\n",
    "# Show distribution of training and validation sets\n",
    "print('Training images: ', train_images.shape)\n",
    "print('Training masks: ', train_masks.shape)\n",
    "print('Validation images: ', val_images.shape)\n",
    "print('Validation masks: ', val_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "851767c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 23:17:18.699411: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 536870912 exceeds 10% of free system memory.\n",
      "2024-10-17 23:17:18.824051: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 536870912 exceeds 10% of free system memory.\n",
      "2024-10-17 23:17:25.325534: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 536870912 exceeds 10% of free system memory.\n",
      "2024-10-17 23:17:26.776203: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 536870912 exceeds 10% of free system memory.\n",
      "2024-10-17 23:17:27.275188: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1073741824 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "#############################################\n",
    "\n",
    "# Implement Early stopping to cut useless epochs\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # or another metric like 'val_accuracy'\n",
    "    patience=hyperparameters['early_stopping_patience'],         # Number of epochs with no improvement before stopping\n",
    "    restore_best_weights=True  # Restores the model to the best state after stopping\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_images, train_masks, \n",
    "                    validation_data=(val_images, val_masks), \n",
    "                    epochs=hyperparameters['epochs'], \n",
    "                    batch_size=hyperparameters['batch_size'],\n",
    "                    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b431a9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322ff4ef-a8b3-4b9f-af7b-cdf0c5437e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Set a larger figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.plot(history.history['loss'], color='blue', linestyle='-', linewidth=2, label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], color='orange', linestyle='--', linewidth=2, label='Validation Loss')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Training and Validation Loss Over Epochs', fontsize=16)\n",
    "plt.xlabel('Epochs', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "\n",
    "# Add a grid for better readability\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add legend to differentiate between training and validation loss\n",
    "plt.legend(loc='best', fontsize=12)\n",
    "\n",
    "# Set limits for better visualization\n",
    "plt.xlim(0, len(history.history['loss']) - 1)  # From epoch 0 to the last epoch\n",
    "plt.ylim(min(history.history['loss']) * 0.95, max(history.history['val_loss']) * 1.05)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
