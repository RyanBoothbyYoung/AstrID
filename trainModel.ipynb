{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f51c6a31-0548-4f9f-b1ca-2652f2caadde",
   "metadata": {},
   "source": [
    "## UnSu Unet Object Localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8c9b41-da1a-431e-8133-f228618ec132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.initializers import he_uniform\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import backend as K\n",
    "import datetime\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d260a505",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8bde07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "K.clear_session()\n",
    "tf.config.experimental.reset_memory_stats('GPU:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b110441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom functions to extract Image arrays and Pixel Mask arrays from our created fits files dataset\n",
    "from dataGathering import extractImageArray, extractPixelMaskArray, extract_star_catalog\n",
    "from dataGathering import getStarData, getImagePlot, getPixelMaskPlot\n",
    "from dataGathering import displayRawImage, displayRawPixelMask, displayImagePlot, displayPixelMaskPlot, displayPixelMaskOverlayPlot\n",
    "\n",
    "# Import custom function to preprocess Image and Pixel Mask arrays\n",
    "from imageProcessing import normalizeImages, stackImages, stackMasks, preprocessImage\n",
    "\n",
    "# Import astropy to read fits files, and os to interact with the file system\n",
    "from astropy.io import fits\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b75e80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getStarData('II/246', 250, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ad1a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create images and masks arrays lists\n",
    "images = []\n",
    "masks = []\n",
    "\n",
    "# Create df to store the star data inside each fits file\n",
    "star_data = []\n",
    "\n",
    "# Create a list of all the fits files in the dataset folder\n",
    "fits_files = os.listdir('data/fits/')\n",
    "\n",
    "# For all the fits files in the dataset folder specified in file_path, extract the image and mask arrays to the respective lists\n",
    "file_path = 'data/fits/'\n",
    "# for file in os.listdir(file_path):\n",
    "for file in os.listdir(file_path):\n",
    "    if file.endswith('.png'):\n",
    "        os.remove(file_path + file)\n",
    "    if file.startswith('data') and file.endswith('.fits'):\n",
    "        images.append(extractImageArray(file_path + file))\n",
    "        masks.append(extractPixelMaskArray(file_path + file))\n",
    "        star_data.append(extract_star_catalog(file_path + file))\n",
    "\n",
    "        print(file + ' added to dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b532c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fits_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c443bc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displayImagePlot(file_path + fits_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00f7d8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displayRawImage(file_path + fits_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01892d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displayRawPixelMask(file_path + fits_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60dab5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displayPixelMaskPlot(file_path + fits_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "388bfa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displayPixelMaskOverlayPlot(file_path + fits_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803fd51f",
   "metadata": {},
   "source": [
    "# Prepare the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e6c296",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "\n",
    "# Define the test size\n",
    "image_parameters = {\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 0,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "images[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6f9d84",
   "metadata": {},
   "source": [
    "## PreProcess images\n",
    "\n",
    "Using a number of techniques from OpenCV we prepare the images for easier training by our ML model\n",
    "- convert_to_grayscale\n",
    "- apply_gaussian_blur\n",
    "- normalize_image\n",
    "- apply_threshold\n",
    "- apply_morphological_operations\n",
    "- normalize_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e61fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0].ndim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81b814e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images_processed = []\n",
    "# for image in images:\n",
    "#     images_processed.append(preprocessImage(image))\n",
    "\n",
    "# images_processed = np.array(images_processed)\n",
    "\n",
    "images_processed = images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91456f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_processed[0].ndim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c650532b",
   "metadata": {},
   "source": [
    "### Convert to 3-Channel Images\n",
    "\n",
    "The images we have are 512 x 512 pixels, but our model requires them to be in the shape `(512, 512, 3)`, similar to standard RGB images. To achieve this, we stack the single-channel images along the last axis three times, converting them into 3-channel images. This transformation is necessary because the model typically expects 3-channel input images.\n",
    "\n",
    "For the masks, the model expects them to be in the shape `(512, 512, 1)`. Therefore, we expand the masks along the last axis to add a new dimension, ensuring they have the correct shape.\n",
    "\n",
    "Additionally, both the images and masks need to be converted to NumPy arrays, as this is the desired format for the training model. Below, we perform these conversions to ensure the data is in the correct format for training.\n",
    "\n",
    "Notice when displaying the shape of the `train_images` list below we see it is an array of 10 images of the shape mentioned above, giving us a shape `(10, 512, 512, 3)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "198982f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert to 3-channel images,   Copilot advises the above (axis=-1) is the correct way to stack the images and masks, if incorrect do below:\n",
    "# ## \"For most convolutional neural networks (CNNs), the expected input shape for images is typically (height, width, channels). Therefore, you should aim to have each image in the shape (512, 512, 3)\" ##\n",
    "# train_images = np.array([np.stack([image, image, image], axis=-1) for image in images])\n",
    "# train_masks = np.array([np.expand_dims(mask, axis=-1) for mask in masks])\n",
    "\n",
    "# #### If the above code does not work, use the below code to stack the images and masks\n",
    "# # train_images = np.array([np.stack([image, image, image], axis=0) for image in images])\n",
    "# # train_masks = np.array([np.stack([mask, mask, mask], axis=0) for mask in masks])\n",
    "\n",
    "\n",
    "\n",
    "# # Convert the list to a NumPy array,   Copilot advises that model training desired \n",
    "# ## \"The model training or validation typically expects a NumPy array of images. The ImageDataGenerator from Keras can accept NumPy arrays as input\" ##\n",
    "# train_images = np.array(train_images)\n",
    "# train_masks = np.array(train_masks)\n",
    "\n",
    "\n",
    "# print(train_images[0].shape)\n",
    "# print(train_images.shape)\n",
    "# print(type(train_images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eee6f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = stackImages(images_processed)\n",
    "train_masks = stackMasks(masks)\n",
    "\n",
    "print(train_images[0].shape)\n",
    "print(train_images.shape)\n",
    "print(type(train_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514f9886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first lines in the first train_images image array\n",
    "train_images[0][0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9801c4f0",
   "metadata": {},
   "source": [
    "### Normalize the Images\n",
    "\n",
    "To standardize the pixel values in our images, we need to normalize them to a common range. Typically, pixel values range from 0 to 255, and normalization is done by dividing each pixel value by 255. However, our images have pixel values that exceed this range.\n",
    "\n",
    "Therefore, we will use min-max normalization to scale the pixel values to a range between 0.0 and 1.0. This involves finding the minimum and maximum pixel values in our dataset and then applying the following formula to each pixel:\n",
    "\n",
    "\\[ \\text{normalized\\_pixel} = \\frac{\\text{pixel} - \\text{min\\_value}}{\\text{max\\_value} - \\text{min\\_value}} \\]\n",
    "\n",
    "This process ensures that all pixel values are standardized, making the data suitable for input into our machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40a1f4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find the minimum and maximum values in the dataset\n",
    "# min_val = np.min(train_images)\n",
    "# max_val = np.max(train_images)\n",
    "\n",
    "# # Apply min-max normalization\n",
    "# train_images_normalized = (train_images - min_val) / (max_val - min_val)\n",
    "\n",
    "# print(train_images_normalized.min(), train_images_normalized.max())  # Should be 0.0 and 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ce79b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_images_normalized = normalizeImages(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fce812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first lines in the first train_images_normalized image array\n",
    "train_images[0][0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4960511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train_images[0][0:1]\n",
    "plt.imshow(train_images[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64281c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "train_images, val_images, train_masks, val_masks = train_test_split(train_images, train_masks, test_size=image_parameters['test_size'], random_state=image_parameters['random_state'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360cc678",
   "metadata": {},
   "source": [
    "### Create `ImageDataGenerator` Instances\n",
    "\n",
    "1. **Create `ImageDataGenerator` for Training Images**: Apply data augmentation.\n",
    "2. **Create `ImageDataGenerator` for Validation Images**: No data augmentation.\n",
    "3. **Create `ImageDataGenerator` for Training Masks**: No data augmentation.\n",
    "4. **Create `ImageDataGenerator` for Validation Masks**: No data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f4b174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ImageDataGenerator for train_image_datagen with data augmentation in order to increase the diversity of the training set\n",
    "train_image_datagen = ImageDataGenerator(\n",
    "    # rescale=1./255,   # No rescale needed as we have already normalized the images\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Use ImageDataGenerator to load data in batches\n",
    "# Create an ImageDataGenerator for validation images without data augmentation\n",
    "val_image_datagen = ImageDataGenerator()\n",
    "\n",
    "# Create an ImageDataGenerator for training masks without data augmentation\n",
    "train_mask_datagen = ImageDataGenerator()\n",
    "\n",
    "# Create an ImageDataGenerator for validation masks without data augmentation\n",
    "val_mask_datagen = ImageDataGenerator()\n",
    "####### generators are set again later after hyperparameters are defined #######\n",
    "\n",
    "\n",
    "# Show distribution of training and validation sets\n",
    "print('Training images: ', train_images.shape)\n",
    "print('Training masks: ', train_masks.shape)\n",
    "print('Validation images: ', val_images.shape)\n",
    "print('Validation masks: ', val_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c32043a-b07f-4121-abd4-45bbdd2db5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder path\n",
    "    #32 kernels\n",
    "    #3x3 kernel size\n",
    "    #padding = same considers edges in the input\n",
    "# Example function to create a U-Net model\n",
    "def unet_model(input_shape, filters, kernel_size, activation, padding, initializer):\n",
    "    inputs = layers.Input(input_shape)\n",
    "\n",
    "    # Encoder path\n",
    "    c1 = layers.Conv2D(filters[0], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(inputs)\n",
    "    c1 = layers.Conv2D(filters[0], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c1)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = layers.Conv2D(filters[1], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(p1)\n",
    "    c2 = layers.Conv2D(filters[1], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c2)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = layers.Conv2D(filters[2], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(p2)\n",
    "    c3 = layers.Conv2D(filters[2], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c3)\n",
    "    p3 = layers.MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    c4 = layers.Conv2D(filters[3], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(p3)\n",
    "    c4 = layers.Conv2D(filters[3], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c4)\n",
    "    p4 = layers.MaxPooling2D((2, 2))(c4)\n",
    "\n",
    "    c5 = layers.Conv2D(filters[4], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(p4)\n",
    "    c5 = layers.Conv2D(filters[4], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c5)\n",
    "\n",
    "    # Decoder path\n",
    "    u6 = layers.Conv2DTranspose(filters[3], (2, 2), strides=(2, 2), padding=padding)(c5)\n",
    "    u6 = layers.concatenate([u6, c4])\n",
    "    c6 = layers.Conv2D(filters[3], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(u6)\n",
    "    c6 = layers.Conv2D(filters[3], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c6)\n",
    "\n",
    "    u7 = layers.Conv2DTranspose(filters[2], (2, 2), strides=(2, 2), padding=padding)(c6)\n",
    "    u7 = layers.concatenate([u7, c3])\n",
    "    c7 = layers.Conv2D(filters[2], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(u7)\n",
    "    c7 = layers.Conv2D(filters[2], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c7)\n",
    "\n",
    "    u8 = layers.Conv2DTranspose(filters[1], (2, 2), strides=(2, 2), padding=padding)(c7)\n",
    "    u8 = layers.concatenate([u8, c2])\n",
    "    c8 = layers.Conv2D(filters[1], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(u8)\n",
    "    c8 = layers.Conv2D(filters[1], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c8)\n",
    "\n",
    "    u9 = layers.Conv2DTranspose(filters[0], (2, 2), strides=(2, 2), padding=padding)(c8)\n",
    "    u9 = layers.concatenate([u9, c1])\n",
    "    c9 = layers.Conv2D(filters[0], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(u9)\n",
    "    c9 = layers.Conv2D(filters[0], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c9)\n",
    "\n",
    "    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "\n",
    "    model = models.Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8beae7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the model for training\n",
    "#############################################\n",
    "\n",
    "# Define the base exponent\n",
    "base_exponent = 5\n",
    "#############################################\n",
    "# base_exponent = 5, so filters result in [32, 64, 128, 256, 512], this was the original value\n",
    "# base_exponent = 6, so filters result in [64, 128, 256, 512, 1024]\n",
    "# base_exponent = 7, so filters result in [128, 256, 512, 1024, 2048]\n",
    "# base_exponent = 8, so filters result in [256, 512, 1024, 2048, 4096]\n",
    "# base_exponent = 9, so filters result in [512, 1024, 2048, 4096, 8192]\n",
    "# base_exponent = 10, so filters result in [1024, 2048, 4096, 8192, 16384]\n",
    "\n",
    "# Generate the filters based on powers of 2\n",
    "filters = [2 ** (base_exponent + i) for i in range(5)]\n",
    "#############################################\n",
    "# This list comprehension generates the filter sizes by raising 2 to the powers starting from the base exponent and increasing by 1 for each subsequent filter.\n",
    "#############################################\n",
    "\n",
    "# Enable mixed precision training\n",
    "# Mixed precision training can help reduce memory usage and speed up training by using both 16-bit and 32-bit floating point types.\n",
    "# tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# Define hyperparameters\n",
    "hyperparameters = {\n",
    "    'input_shape': (512, 512, 3),\n",
    "    'filters': filters,\n",
    "    'kernel_size': (3, 3),\n",
    "    'activation': 'relu',\n",
    "    'padding': 'same',\n",
    "    'initializer': he_uniform(),\n",
    "    'optimizer': 'adam',\n",
    "    'loss': 'binary_crossentropy',\n",
    "    'metrics': ['accuracy'],\n",
    "    'epochs': 1000,\n",
    "    'batch_size': 4,\n",
    "    'validation_split': image_parameters['test_size'],\n",
    "    'early_stopping_patience': 10\n",
    "}\n",
    "\n",
    "# Create and compile the model using hyperparameters\n",
    "model = unet_model(\n",
    "    input_shape=hyperparameters['input_shape'],\n",
    "    filters=hyperparameters['filters'],\n",
    "    kernel_size=hyperparameters['kernel_size'],\n",
    "    activation=hyperparameters['activation'],\n",
    "    padding=hyperparameters['padding'],\n",
    "    initializer=hyperparameters['initializer']\n",
    ")\n",
    "model.compile(\n",
    "    optimizer=hyperparameters['optimizer'],\n",
    "    loss=hyperparameters['loss'],\n",
    "    metrics=hyperparameters['metrics']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8932a853",
   "metadata": {},
   "source": [
    "**Create Generators for Training and Validation**:\n",
    "   - Use the `flow` method to create generators for training and validation images and masks.\n",
    "   - Ensure that the `seed` parameter is the same for both image and mask generators to maintain alignment.\n",
    "\n",
    "7. **Custom Generator**:\n",
    "   - Define a custom generator function `custom_generator` that yields batches of images and masks together.\n",
    "   - Use `zip` to combine the image and mask generators for both training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0b4e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b654720e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generators for training and validation images\n",
    "train_image_generator = train_image_datagen.flow(\n",
    "    train_images, batch_size=hyperparameters['batch_size'], seed=image_parameters['seed']\n",
    ")\n",
    "\n",
    "val_image_generator = val_image_datagen.flow(\n",
    "    val_images, batch_size=hyperparameters['batch_size'], seed=image_parameters['seed']\n",
    ")\n",
    "\n",
    "# Create generators for training and validation masks\n",
    "train_mask_generator = train_mask_datagen.flow(\n",
    "    train_masks, batch_size=hyperparameters['batch_size'], seed=image_parameters['seed']\n",
    ")\n",
    "\n",
    "val_mask_generator = val_mask_datagen.flow(\n",
    "    val_masks, batch_size=hyperparameters['batch_size'], seed=image_parameters['seed']\n",
    ")\n",
    "\n",
    "# Custom generator to yield images and masks together\n",
    "def custom_generator(image_generator, mask_generator):\n",
    "    while True:\n",
    "        image_batch = next(image_generator)\n",
    "        mask_batch = next(mask_generator)\n",
    "        yield (image_batch, mask_batch)\n",
    "\n",
    "# Combine the image and mask generators\n",
    "train_generator = custom_generator(train_image_generator, train_mask_generator)\n",
    "val_generator = custom_generator(val_image_generator, val_mask_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7f8de9",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851767c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#############################################\n",
    "\n",
    "# Implement Early stopping to cut useless epochs\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # or another metric like 'val_accuracy'\n",
    "    patience=hyperparameters['early_stopping_patience'],         # Number of epochs with no improvement before stopping\n",
    "    restore_best_weights=True  # Restores the model to the best state after stopping\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_generator, \n",
    "                    validation_data=val_generator,\n",
    "                    epochs=hyperparameters['epochs'],\n",
    "                    steps_per_epoch=len(train_images) // hyperparameters['batch_size'],\n",
    "                    validation_steps=len(val_images) // hyperparameters['batch_size'],\n",
    "                    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd42b3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "saved_models_path = 'models/saved_models/'\n",
    "training_size = str(len(train_images))\n",
    "saved_model_name = datetime.datetime.now().strftime(\"%Y_%m_%d-%H%M_\") + training_size + '_unet_model.keras'\n",
    "model.save(saved_models_path + saved_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9056adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show training loss and validation loss\n",
    "print('Training loss: ', history.history['loss'][-1])\n",
    "print('Validation loss: ', history.history['val_loss'][-1])\n",
    "\n",
    "# Show training accuracy and validation accuracy\n",
    "print('Training accuracy: ', history.history['accuracy'][-1])\n",
    "print('Validation accuracy: ', history.history['val_accuracy'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b431a9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(24, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/figures/' + saved_model_name.removesuffix('unet_model.keras') + '_training_validation.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322ff4ef-a8b3-4b9f-af7b-cdf0c5437e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a larger figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.plot(history.history['loss'], color='blue', linestyle='-', linewidth=2, label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], color='orange', linestyle='--', linewidth=2, label='Validation Loss')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Training and Validation Loss Over Epochs', fontsize=16)\n",
    "plt.xlabel('Epochs', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "\n",
    "# Add a grid for better readability\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add legend to differentiate between training and validation loss\n",
    "plt.legend(loc='best', fontsize=12)\n",
    "\n",
    "# Set limits for better visualization\n",
    "plt.xlim(0, len(history.history['loss']) - 1)  # From epoch 0 to the last epoch\n",
    "plt.ylim(min(history.history['loss']) * 0.95, max(history.history['val_loss']) * 1.05)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/figures/' + saved_model_name.removesuffix('unet_model.keras') + '_training_validation_loss.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
